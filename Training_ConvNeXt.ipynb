{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lolikgiovi/ConvNeXt-Repro/blob/main/Training_ConvNeXt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLDhghC4ZScc"
      },
      "source": [
        "## Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1NHh8vo6HsF"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install timm==0.3.2 tensorboardX six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfHP00nmNvj2"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znP2jlYfYPEv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Using Weights a& Biases Dashboard\n",
        "I found it convenient to monitor my model training performance via W&B Dashboard. You can log in to W&B using this command and follow the instruction through."
      ],
      "metadata": {
        "id": "NG1rusYIWIMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1178f6d81bafb6a3f9362a927de31ed685ab4c59\n",
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "shGZtSWkMe7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT6CasRLZZKW"
      },
      "source": [
        "## Dataset\n",
        "I am using [Imagenette Dataset](https://github.com/fastai/imagenette). It is a **subset of Imagenet** dataset, the dataset being used in [the official ConvNeXt implementation](https://github.com/facebookresearch/ConvNeXt).\n",
        "\n",
        "Both of the dataset containing images with Fullsize, 320px and 160px size. I am using the 160px for training the ConvNeXt model.\n",
        "\n",
        "The dataset also comes with a CSV file with 1%, 5%, 25%, and 50% of the labels randomly changed to an incorrect label.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tCqILVtanjC"
      },
      "source": [
        "### Imagenette\n",
        "*Imagenette* is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fis649mas-X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e043054a-24d3-40ae-93e7-e4e1b2977ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-05 15:38:18--  https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.165.160, 52.217.118.160, 52.217.101.110, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.165.160|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 99003388 (94M) [application/x-tar]\n",
            "Saving to: ‘imagenette2-160.tgz’\n",
            "\n",
            "imagenette2-160.tgz 100%[===================>]  94.42M  26.8MB/s    in 3.5s    \n",
            "\n",
            "2023-03-05 15:38:22 (26.8 MB/s) - ‘imagenette2-160.tgz’ saved [99003388/99003388]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Getting data from Imagenette\n",
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
        "!tar -xzf imagenette2-160.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YneHUU_BaVwf"
      },
      "source": [
        "### Alternative: Imagewoof\n",
        "*Imagewoof* is a subset of 10 classes from Imagenet **that aren't so easy to classify**, since they're all dog breeds. The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG6uNmu_6NnL"
      },
      "outputs": [],
      "source": [
        "# Getting data from Imagewoof\n",
        "!wget https://s3.amazonaws.com/fast-ai-imageclas/imagewoof2-160.tgz\n",
        "!tar -xvzf imagewoof2-160.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HmeCL0nPqIq"
      },
      "source": [
        "## Setting up Model Training in Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq763_bgYW6m"
      },
      "source": [
        "Original training command [from repo](https://github.com/facebookresearch/ConvNeXt/blob/main/TRAINING.md):\n",
        "\n",
        "\n",
        "```\n",
        "python -m torch.distributed.launch --nproc_per_node=8 main.py \\\n",
        "                                   --model convnext_tiny --drop_path 0.1 \\\n",
        "                                   --batch_size 128 --lr 4e-3 --update_freq 4 \\\n",
        "                                   --model_ema true --model_ema_eval true \\\n",
        "                                   --data_path /path/to/imagenet-1k \n",
        "                                   --output_dir /path/to/save_results\n",
        "```\n",
        "\n",
        "Using this command straight up in my Google Colab, it will resulting error like:\n",
        "```\n",
        "RuntimeError: CUDA error: invalid device ordinal  File \"main.py\", line 477, in <module>\n",
        "```\n",
        "\n",
        "So I specified the CUDA Device first and changed the nproc_per_node from 8 to 1, my training command become:\n",
        "```\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 main.py \\\n",
        "                                    --model convnext_tiny --drop_path 0.1 \\\n",
        "                                    --batch_size 128 --lr 4e-3 --update_freq 4 \\\n",
        "                                    --model_ema true --model_ema_eval true \\\n",
        "                                    --input_size 160 --drop_path 0.2 \\\n",
        "                                    --data_path /content/imagenette2-160 \\\n",
        "                                    --output_dir /content/res\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEeKMWF06X9x",
        "outputId": "e48ee53e-b9be-4182-a622-080626af11d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ConvNeXt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ConvNeXt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using ConvNeXt Tiny as the model architecture, since my task is requiring me to train the models using smallest dataset, then a smaller architecture will fit best since it have fewer parameters and require less data to train."
      ],
      "metadata": {
        "id": "ZPeuTOAALyGS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmUA0BePRAQm"
      },
      "source": [
        "### ConvNeXt-T -- Batch 32, Augmentation Default\n",
        "- Batch size: 32\n",
        "- Epochs: 100\n",
        "- Update Freq: 4\n",
        "- Input Size: 160 (Imagenette2-160)\n",
        "- Learning rate: 0.004\n",
        "- Drop: 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my first trial on Training the model. I tried to train 100 epoch with 50-30-20 steps since I want to see the initial performance first before doing the whole 100 epochs. "
      ],
      "metadata": {
        "id": "QAAzsbiAXFWX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZkrN_Cq7jI0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/result_tiny\n",
        "%cd /content/ConvNeXt\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 main.py \\\n",
        "                                    --model convnext_tiny \\\n",
        "                                    --epochs 100 \\\n",
        "                                    --batch_size 32 \\\n",
        "                                    --lr 4e-3 \\\n",
        "                                    --update_freq 4 \\\n",
        "                                    --model_ema true \\\n",
        "                                    --model_ema_eval true \\\n",
        "                                    --aa original \\\n",
        "                                    --drop_path 0.1 \\\n",
        "                                    --opt adamw \\\n",
        "                                    --train_interpolation bicubic \\\n",
        "                                    --input_size 160 \\\n",
        "                                    --data_path /content/imagenette2-160 \\\n",
        "                                    --output_dir /content/result_tiny \\\n",
        "                                    --log_dir /content/result_tiny \\\n",
        "                                    --enable_wandb true --wandb_ckpt true"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "!python main.py --model convnext_tiny --eval true \\\n",
        "                --resume /content/result_tiny/checkpoint-best.pth \\\n",
        "                --input_size 160 --drop_path 0.1 \\\n",
        "                --data_path /content/imagenette2-160"
      ],
      "metadata": {
        "id": "gZIKd8PwVTvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConvNeXt-T -- Batch 64, Augmentation Default\n",
        "- Batch size: 64\n",
        "- Epochs: 100\n",
        "- Update Freq: 4\n",
        "- Input Size: 160 (Imagenette2-160)\n",
        "- Learning rate: 0.004\n",
        "- Drop: 0.2\n"
      ],
      "metadata": {
        "id": "LXN_bF2KnQlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this approach, I tried to make the batch size bigger so the training will be stable. It might be more stable though since the Acc@1 EMA is the highest among all, but the Acc@1 is considered smaller than the ones with smaller batch size."
      ],
      "metadata": {
        "id": "5oizDIaYXout"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/result_tiny2\n",
        "%cd /content/ConvNeXt\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 main.py \\\n",
        "                                    --model convnext_tiny \\\n",
        "                                    --epochs 100 \\\n",
        "                                    --batch_size 64 \\\n",
        "                                    --lr 4e-3 \\\n",
        "                                    --update_freq 4 \\\n",
        "                                    --model_ema true \\\n",
        "                                    --model_ema_eval true \\\n",
        "                                    --aa original \\\n",
        "                                    --drop_path 0.1 \\\n",
        "                                    --opt adamw \\\n",
        "                                    --train_interpolation bicubic \\\n",
        "                                    --input_size 160 \\\n",
        "                                    --data_path /content/imagenette2-160 \\\n",
        "                                    --nb_classes 10 \\\n",
        "                                    --output_dir /content/result_tiny2 \\\n",
        "                                    --log_dir /content/result_tiny2 \\\n",
        "                                    --enable_wandb true --wandb_ckpt true"
      ],
      "metadata": {
        "id": "cHwcOpTjnPtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "!python main.py --model convnext_tiny --eval true \\\n",
        "                --resume /content/result_tiny2/checkpoint-best.pth \\\n",
        "                --input_size 160 --drop_path 0.1 \\\n",
        "                --data_path /content/imagenette2-160"
      ],
      "metadata": {
        "id": "-6Y3lTCv90Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConvNeXt-T -- Batch 32, Augmentation Modified\n",
        "- Batch size: 32\n",
        "- Epochs: 100\n",
        "- Update Freq: 4\n",
        "- Input Size: 160 (Imagenette2-160)\n",
        "- Learning rate: 0.004\n",
        "- Drop: 0.2\n",
        "\n",
        "Augmentation Edit:\n",
        "- color_jitter: 0.5 (default: 0.4)\n",
        "- smoothing: 0.2 (default: 0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "n7zxkRNVZPl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I tried to get back with 32 Batch Size but modified the augmentation variable a bit. The result is the highest amongst all."
      ],
      "metadata": {
        "id": "x67uzt5AZHH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/result_tiny3\n",
        "%cd /content/ConvNeXt\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 main.py \\\n",
        "                                    --model convnext_tiny \\\n",
        "                                    --epochs 100 \\\n",
        "                                    --batch_size 32 \\\n",
        "                                    --lr 4e-3 \\\n",
        "                                    --update_freq 4 \\\n",
        "                                    --model_ema true \\\n",
        "                                    --model_ema_eval true \\\n",
        "                                    --aa original \\\n",
        "                                    --drop_path 0.1 \\\n",
        "                                    --color_jitter 0.5 \\\n",
        "                                    --smoothing 0.2 \\\n",
        "                                    --opt adamw \\\n",
        "                                    --train_interpolation bicubic \\\n",
        "                                    --input_size 160 \\\n",
        "                                    --data_path /content/imagenette2-160 \\\n",
        "                                    --nb_classes 10 \\\n",
        "                                    --output_dir /content/result_tiny3 \\\n",
        "                                    --log_dir /content/result_tiny3 \\\n",
        "                                    --enable_wandb true --wandb_ckpt true"
      ],
      "metadata": {
        "id": "l-a0NTE7bQDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "!python main.py --model convnext_tiny --eval true \\\n",
        "                --resume /content/result_tiny3/checkpoint-best.pth \\\n",
        "                --input_size 160 --drop_path 0.1 \\\n",
        "                --data_path /content/imagenette2-160"
      ],
      "metadata": {
        "id": "tDFUmPLgLKNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ConvNeXt-S -- Batch 32, Augmentation Default\n",
        "- Batch size: 32\n",
        "- Epochs: 100\n",
        "- Update Freq: 4\n",
        "- Input Size: 160 (Imagenette2-160)\n",
        "- Learning rate: 0.004\n",
        "- Drop: 0.2"
      ],
      "metadata": {
        "id": "ubE0G9sZNYOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/result_small_1\n",
        "%cd /content/ConvNeXt\n",
        "!CUDA_VISIBLE_DEVICES=0 python -m torch.distributed.launch --nproc_per_node=1 main.py \\\n",
        "                                    --model convnext_small \\\n",
        "                                    --epochs 100 \\\n",
        "                                    --batch_size 64 \\\n",
        "                                    --lr 4e-3 \\\n",
        "                                    --update_freq 4 \\\n",
        "                                    --model_ema true \\\n",
        "                                    --model_ema_eval true \\\n",
        "                                    --aa original \\\n",
        "                                    --drop_path 0.1 \\\n",
        "                                    --opt adamw \\\n",
        "                                    --train_interpolation bicubic \\\n",
        "                                    --input_size 160 \\\n",
        "                                    --data_path /content/imagenette2-160 \\\n",
        "                                    --nb_classes 10 \\\n",
        "                                    --output_dir /content/result_small_1 \\\n",
        "                                    --log_dir /content/result_small_1 \\\n",
        "                                    --enable_wandb true --wandb_ckpt true"
      ],
      "metadata": {
        "id": "-BgR8xGXNbyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "!python main.py --model convnext_small --eval true \\\n",
        "                --resume /content/result_small_1/checkpoint-best.pth \\\n",
        "                --input_size 160 --drop_path 0.1 \\\n",
        "                --data_path /content/imagenette2-160"
      ],
      "metadata": {
        "id": "VgiPPQ4aOePB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6tCqILVtanjC",
        "YneHUU_BaVwf"
      ],
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1TVH81eu5rXYt4lHGUgylNyCTU6q0a5s0",
      "authorship_tag": "ABX9TyOOkcQMfG5k2rwrphyPGIa9",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}